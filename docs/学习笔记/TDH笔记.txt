创建内外表：
create table hbase_inner_table(
  key1 string,
  bi bigint,
  dc decimal(10,2),
  ch varchar(10),
  ts timestamp,
  en string)STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler';
  
CREATE EXTERNAL TABLE hbase_external_table(
    key1 string,
    ex1 double,
    ex3 date,
    ex5 string
)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,f:q1,f:q4,f:q5") 
TBLPROPERTIES ("hbase.table.name"="assetmap.hbase_inner_table");

BATCHINSERT INTO hbase_inner_table BATCHVALUES (
VALUES('004',4,4.01,'esdrive','2017-01-11 15:05:20','wednesday'),
VALUES('005',5,5.01,'transwarp es','2017-01-12 15:18:18','thursday'),
VALUES('006',6,6.01,'hyperdrive','2017-01-13 05:13:13','friday'),
VALUES('007',7,7.01,'inceptor','2017-01-14 10:55:20','saturday'),
VALUES('008',8,8.01,'fulltext','2017-01-15 17:23:40','tuestuesday')
);

Kafka：
创建topic：./kafka-topics.sh  --zookeeper 10.64.141.33:2181 --create --topic demo-test --partitions 3 --replication-factor 1
查看topic：./kafka-topics.sh  --zookeeper 10.64.141.33:2181 --describe --topic demo


建Stream及触发StreamJob:
连接到StreamJob：beeline  -u "jdbc:hive2://bak-BigDataNode01:10010" -n hive -p hive
建Stream：CREATE STREAM demo_stream(id INT, letter STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' TBLPROPERTIES("topic"="kafka-topic","kafka.zookeeper"="10.128.40.165:2181","kafka.broker.list"="10.128.40.163:9092");
查看Stream：SHOW STREAMS;
创建StreamJob：CREATE TABLE demo_table(id INT, letter STRING);
操作:INSERT INTO demo_table SELECT * FROM demo_stream;


CREATE DATABASE IF NOT EXISTS exchange_platform;

Kafka测试：
kafka接收slipstream数据操作步骤：
https://app.yinxiang.com/fx/dc181643-dc8f-429e-b589-4230c97ab3b3

export KAFKA_OPTS="-Djava.security.auth.login.config=/opt/tdhadmin/jaas.conf -Djava.security.krb5.conf=/opt/tdhadmin/krb5.conf -Dsun.security.krb5.debug=true"

创建topic：
./kafka-broker-topics.sh  --bootstrap-server 10.128.40.163:9092 --create --topic demo --replication-factor 1 --partitions 1 --consumer-property security.protocol=SASL_PLAINTEXT --consumer-property sasl.kerberos.service.name=kafka
生产者：
./kafka-console-producer.sh --broker-list 10.128.40.163:9092 --topic demo --producer.config /home/tdhadmin/TDH-Client/kafka/config/producer.properties
消费者：
./kafka-console-consumer.sh --bootstrap-server 10.128.40.163:9092 --from-beginning --topic demo --consumer.config /home/tdhadmin/TDH-Client/kafka/config/consumer.properties

SlipStream测试：
beeline  -u "jdbc:hive2://10.128.40.164:10010" -n tdhadmin -p tdhadmin

CREATE STREAM demo_stream(id INT, letter STRING)
  ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
  TBLPROPERTIES("topic"="demo",
  "kafka.zookeeper"="localhost:2181",
  "kafka.broker.list"="10.128.40.163:9092",
  "transwarp.consumer.security.protocol"="SASL_PLAINTEXT",
  "transwarp.consumer.sasl.kerberos.service.name"="kafka",
   "transwarp.consumer.sasl.jaas.config"="com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true storeKey=true  keyTab=\"/etc/slipstream1/conf/tdhadmin.keytab\" principal=\"tdhadmin@GDTDH\""
  );
  
CREATE TABLE demo_table(id INT, letter STRING);
INSERT INTO demo_table SELECT * FROM demo_stream;
LIST STREAMJOBS;
SELECT * FROM demo_table;


